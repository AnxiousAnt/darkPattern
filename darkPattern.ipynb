{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "!pip install beautifulsoup4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdCch27hejkF",
        "outputId": "f92a6edb-6f1b-4118-b50c-cc3abb7be2e6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.11.17)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca5U18mNd_AC",
        "outputId": "019385af-bc13-4eee-a390-3134d33c581e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The website at https://www.amazon.in/ does not contain false urgency.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Your text preprocessing logic here\n",
        "    return text\n",
        "\n",
        "def extract_text_from_website(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        paragraphs = soup.find_all('p')\n",
        "        text_content = ' '.join([p.get_text() for p in paragraphs])\n",
        "        return text_content\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching content from {url}: {e}\")\n",
        "        return ''\n",
        "\n",
        "data = [\n",
        "    {\"text\": \"Limited-time offer! Act now!\", \"label\": 1},\n",
        "    {\"text\": \"Our products are always available.\", \"label\": 0},\n",
        "]\n",
        "\n",
        "for example in data:\n",
        "    example['text'] = preprocess_text(example['text'])\n",
        "\n",
        "X = [example['text'] for example in data]\n",
        "y = [example['label'] for example in data]\n",
        "\n",
        "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
        "model.fit(X, y)\n",
        "\n",
        "def analyze_website(url):\n",
        "    website_text = extract_text_from_website(url)\n",
        "    website_text = preprocess_text(website_text)\n",
        "    prediction = model.predict([website_text])[0]\n",
        "    if prediction == 1:\n",
        "        print(f\"The website at {url} contains false urgency.\")\n",
        "    else:\n",
        "        print(f\"The website at {url} does not contain false urgency.\")\n",
        "\n",
        "# Example website URL\n",
        "website_url = 'https://www.amazon.in/'\n",
        "\n",
        "# Analyze the website\n",
        "analyze_website(website_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "24JabBFAeNHf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}